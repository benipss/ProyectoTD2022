---
title: "PROYECTO TD 2022"
author: "Benito Pastor Sánchez, Carlos Heras Pardo, Hugo Toledo Escrivá y Manuel Perez Perdomo"
date: "`r Sys.Date()`"
output:
  # pdf_document:
  #   toc: yes
  #   toc_depth: 3
  #   number_sections: no
  html_document:
    echo: yes
    number_sections: no
    theme: lumen
    toc: yes
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}


# CONFIGURACIÓN GENERAL
library(knitr)
options(width = 100)

# Opciones generales de los chucks. Se utilizarán salvo cambios en el chunk
knitr::opts_chunk$set(
	echo = TRUE,
	error = F,
	fig.align = "center",
	fig.path = "./figura/",
	message = FALSE,
	warning = FALSE,
	cache.path = ".cache/",
	comment = NA,
	dpi = 200,
	tidy = F
)
# Opciones generales de dígitos cuando se incluyen tablas
#options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
#knit_hooks$set(plot = knitr:::hook_plot_html)
```

# Instalación automática de paquetes

```{r warning=FALSE, include=FALSE}

# Especificamos las librerías necesarias en esta lista

packages = c("tidyverse","knitr","dplyr","mice","ggplot2","GGally")

#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(packages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE,repos='http://cran.rediris.es')
  }
  library(x, character.only = TRUE)
})

#verify they are loaded
search()

```

# Introducción del trabajo

Breve introducción donde se indica el contenido del documento:

" El objetivo de este proyecto es abordar un problema de tratamiento de datos que abarque todas las etapas que estamos estudiando a lo largo del curso. En este proyecto analizaremos los datos recogidos por sensores que monitorizan el nivel de ruido en diferentes localizaciones del barrio de Ruzafa. Los datos están disponibles en la plataforma de datos abiertos del Ayuntamiento de Valencia, dentro de la categoría medio ambiente"

# Importacion de los datos.

```{r}
ruta <- "data/"
f <- list.files(path= "./data", pattern = "csv$") 
fusion <- read_csv(paste(ruta,f[1], sep = ""), 
     col_types = cols(`_id` = col_integer(), 
         LAeq = col_number(), LAeq_d = col_number(), 
         LAeq_den = col_number(), LAeq_e = col_number(), 
         LAeq_n = col_number(), dateObserved = col_date(format = "%Y-%m-%d")))%>% mutate(calle = 1)
     
for (i in 2:length(f)){
fichero2 <- read_csv(paste(ruta,f[i], sep = ""),
       col_types = cols(`_id` = col_integer(), 
           LAeq = col_number(), LAeq_d = col_number(), 
           LAeq_den = col_number(), LAeq_e = col_number(), 
           LAeq_n = col_number(), dateObserved = col_date(format = "%Y-%m-%d"))) %>% mutate(calle = i) 

fusion <- union(fusion, fichero2)
 
}

```

# Acondicionamiento de los datos.

```{r}
#Eliminamos las columnas 3, 4 y 5.
fusion <- fusion %>% select(-(3:5)) %>% select(-1)
#Corregimos la comulna de identificacion "_id".
numero <- nrow(fusion)
fusion <- fusion %>% mutate("id" = c(1:numero)) %>% select(-(1))
```

# Detección de NA.

```{r}
#Calculamos el número de NA que existen en nustro conjunto de observaciones.
num_nona <- sum(complete.cases(fusion) == TRUE)
num_na <- nrow(fusion) - num_nona
num_na
#Calculamos el porcentaje de NA respecto de todos los datos.
((num_na/nrow(fusion))*100)

#Repetimos el proceso unicamente teniendo en cuenta las variables numéricas.
df_var_num <- fusion %>% select(1:5)
sum(complete.cases(df_var_num) == TRUE)
```

Ya que el numero de observaciones con valores perdidos es muy bajo y estos pertenecen a variables númericas, hemos decidido sustituirlos por un estadístico "robusto" como es la media.

```{r include=FALSE}
#Imputamos los NA.
df_var_num <- mice(df_var_num,m=5,method="mean")
summary(df_var_num)

#Transformamos la lista que nos devuelve el mice a un data frame de nuevo
df_var_num <- complete(df_var_num)

#Comprobamos que hayamos imputado correctamente todos los datos 
sum(complete.cases(df_var_num) == TRUE) == nrow(df_var_num)

```

# Analisis Univariante

```{r}
# Aplicamos la regla boxplot para eliminar los datos considerados outliers.

reglaboxplot <- function(x,na.rm=T) { 
  q3 = quantile(x,0.75,na.rm = na.rm)
  q1 = quantile(x,0.25,na.rm = na.rm)
  rangoiqr = IQR(x,na.rm = na.rm)
  outliers = (x > q3+1.5*rangoiqr) | (x < q1-1.5*rangoiqr)
  return(outliers)
}

#Escogemos unicamente las variables numericas de data frame.
A <-lapply(df_var_num, reglaboxplot)
nombre_var <- names(df_var_num)


#Contamos los outliers de todo el dataframe.
numero = 0
for (x in 1:length(nombre_var)){
numero1 = sum(A[[nombre_var[x]]])
numero = numero + numero1
}
numero

#Calculamos el porcentaje de outliers del conjunto de datos.
100 * (numero / (nrow(df_var_num) * ncol(df_var_num))) 

```

Podemos observar que el porcentaje de outliers sobre todo el conjunto de datos es del 4.27%.

Procedemos a representar los outliers mediante un gráfico boxplot para poder visualizarlo mejor.

```{r}
#Transformamos el conjunto de datos en un conjunto tidy para poder representar las variables mediante la función pivor_longer:
df_var_num_tidy <- df_var_num %>% pivot_longer(cols=c(1:ncol(df_var_num)),names_to= "Nombre_var")

df_var_num_tidy %>% ggplot(aes(x=Nombre_var, y = value, col = Nombre_var)) + geom_boxplot()  

```

Procedemos a calcular los siguientes estadísticos:

-   Media

-   Mediana

-   Desviación Típica

-   IQR (Rango Intercuartil)

```{r}

df_estadisticos <- df_var_num %>% summarise(media = mean(df_var_num[,1]),sd = sd(df_var_num[,1]),mediana =  median(df_var_num[,1]),IQR = IQR(df_var_num[,1])) %>%  round(2)

for (i in 2:ncol(df_var_num)){
estadisticos <-  df_var_num %>% summarise(media = mean(df_var_num[,i]),sd = sd(df_var_num[,i]),mediana =  median(df_var_num[,i]),IQR = IQR(df_var_num[,i])) %>% round(2)
 df_estadisticos <- union(df_estadisticos, estadisticos)
}

df_estadisticos <- df_estadisticos %>% mutate("Nom_Var" = colnames(df_var_num)) %>% kable()

df_estadisticos
```

# Análisis Bivariante

Calculamos las matrices de covarianza (no acotada) y correlación (acotada), considerando todas las variables numéricas, para encontrar relaciones (lineales) entre variables.

```{r}
df_var_num %>% cov() %>% round(2)
#Calculamos la matriz de correlación con el método de Pearson:
matriz_Corr_pearson = df_var_num %>% cor(method="pearson") %>% round(2)
matriz_Corr_pearson
#Representación de la matriz de correlación:
ggcorr(matriz_Corr_pearson)
#Calculamos la matriz de correlación con el método de Spearman:
matriz_Corr_spearman = df_var_num %>% cor(method="spearman") %>% round(2)
matriz_Corr_spearman
#Representación de la matriz de correlación:
ggcorr(matriz_Corr_spearman)
```

# Contextualización de los datos.

## Distribucion del ruido a lo largo de los años en las 3 franjas horarias (7h-19h,19h-23h,23h-7h)

```{r}
#Seleccionamos las columnas necesarias:
año <- fusion %>% select("dateObserved") %>% separate(dateObserved,c("año","mes","dia"),sep = "-") %>% select(año)

P1 <- cbind(df_var_num,año)
P1 <- P1 %>% group_by(año) %>% 
  summarise(media_d = mean(LAeq_d),media_e = mean(LAeq_e),media_n = mean(LAeq_n)) %>% pivot_longer(cols = c(2:4), names_to = "Medias") 

ggplot(data=P1, aes(x=año, y=value, group=Medias)) +
  geom_line(aes(linetype=Medias,color=Medias))+
  geom_point(aes(shape=Medias,color=Medias))

```
Conclusión : El ruido tomando como año inicial en el que se empezaron a recoger datos (2020) hasta la actualidad (2022), ha aumentado considerablemente como podemos ver en la anterior gráfica, en la que la pendiente en el tramo de 2021 a 2022 se pronuncia claramente.


